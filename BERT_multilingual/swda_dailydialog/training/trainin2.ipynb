from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"




import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW
# from torch.optim import Adam
import torch.nn.functional as F
import time
import os
import datetime
from datetime import datetime 
from transformers import get_linear_schedule_with_warmup
import random # 재현을 위해 랜덤시드 고정

# default_directory = './saved_model'
date = datetime.now()
work_date=str(date.year)+str(date.month)+str(date.day)

# 저장위치 확인
model_save_dir = './saved_model/'  +work_date #./saved_model/2021712

if not os.path.exists(model_save_dir):
    os.makedirs(model_save_dir)

# Setting parameters
batch_size = 32
num_label=3
learning_rate = 2e-6
max_grad_norm = 1
epochs = 10
# max_len = 512

start_time = time.time()

# train_df = 함수 실행

train_df=pd.read_csv('../dataset/speechact_train.csv') #sep='\t')
valid_df=pd.read_csv('../dataset/speechact_valid.csv') #sep='\t')

# test_df = pd.read_csv('./nsmc/ratings_test.txt', sep='\t')
train_df[:1]  #1000 rows × 2 columns




a=list(train_df['train_sentences'])
for i in a:
    if '\\' in i:
        print(i)




class NsmcDataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        text = self.df.iloc[idx, 0]
#         print(text)
        
        label = self.df.iloc[idx, 1]
#         print(label)
        return text, label

train_dataset = NsmcDataset(train_df)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)

valid_dataset = NsmcDataset(valid_df)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=2)

# test_dataset = NsmcDataset(test_df)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2)




# text=test_dataset.__getitem__(0)

# for i in range(len(test_dataset)):
#     text, label = test_dataset.__getitem__(i)
# #     print(text, "=======", label)




device = torch.device("cuda")

tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased') #stride=256, return_overflowing_tokens=True, do_lower_case=False)

model = BertForSequenceClassification.from_pretrained("bert-base-multilingual-cased", num_labels=num_label)
# model_save_dir = '/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/swda_dailydialog_3acts/ACT_token/_saved_model/20210615/epoch=5.pt'
#num_features = model.classifier.in_features
#model.classifier = nn.Linear(num_features, 3)


# 스페셜 토큰 ACT, TOP 추가
special_tokens_dict = {"additional_special_tokens": [ "[ACT]", "[TOP]"] }
num_added_special_tokens = tokenizer.add_special_tokens(special_tokens_dict)
print('sepecial_tokens', num_added_special_tokens)

# new토큰 추가
new_tokens =["글쎄", "어딨", "어땠", "뭘", "좀", "눈썹", "고맙", "깜빡", "알아챘", "어둡", "칵테일", "둬", "흠", "알잖", "어쨌", "샴푸", "부엌", "긁힌", "펍", "말했", "말썽",
"봅", "비쌉니까", "귤", "짭짤", "덥", "안팎", "뚜껑", "여깄", "여쭤", "눕혀", "볶음", "새우", "아닙", "샵", "젓가락", "숟가락", "압력", "솥", "생선", "찜", "말했",
"배고팠", "땀", "조깅", "짚었", "잤", "여깄", "돼지", "고기", "팁", "예약", "해놨", "바쁠", "깡충", "깡통", "냅킨", "봅", "찐빵", "뵙", "맙소사", "즙", "헷갈",
"뒹굴", "가엾", "찻집", "으깬", "쿵푸", "얌전", "푹", "뽑", "베깅", "핏", "놀랐", "꽉", "춥다", "꽂", "이퀄라이제이션", "놔", "켤레", "벙어리", "샴페인", "쪄", "가오슝",
"씻", "덫", "바빴", "뻣뻣", "찍어", "개", "똥", "엎", "뺄", "볶", "찐", "웍" "놔", "멈췄", "닳", "푹", "찻", "댁", "쪘", "있", "쬐", "잖아"]
num_added_new_tokens  = tokenizer.add_tokens(new_tokens)
print('new_tokens', num_added_new_tokens)


#처음 vocab_size
print('추가 전', model.get_input_embeddings())  #Embedding(119547, 768, padding_idx=0) #tokenizer.vocab_size 

total_added_tokens = num_added_special_tokens + num_added_new_tokens
print('총 추가된 토큰 수', total_added_tokens)

model.resize_token_embeddings(tokenizer.vocab_size + total_added_tokens)
print('토큰 추가 후', model.get_input_embeddings())  #토큰 추가 후 Embedding(119646, 768)

model.to(device)

# tokenizer.special_tokens_map

optimizer = AdamW(model.parameters(),
                  lr=learning_rate, eps=1e-8
                  )

total_steps = len(train_loader) * epochs  # 학습률을 조금씩 감소시키는 스케줄러 생성
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)



seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# checkpoint = torch.load(model_save_dir)
# print(checkpoint.keys())

# epoch = checkpoint['epoch']
# model.load_state_dict(checkpoint['model_state_dict'])
# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
# print('Model loaded!')
# model.to(device)




def train(epoch):

    model.train()
    total_loss = 0
    total_len = 0
    total_correct = 0
    itr = 1
    p_itr = 500

    for text, label in train_loader:
#         print('Training...')


        optimizer.zero_grad()
        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]
        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]
        sample = torch.tensor(padded_list)
        sample, label = sample.to(device), label.to(device)
        labels = label.clone().detach()
        outputs = model(sample, labels=labels)
#         print('outputs.loss', outputs.loss)
#         print('outputs.logits', outputs.logits)
        loss, logits = outputs
#         print('logits', logits)  #tensor([[0.0947, 0.2428, 0.1082]]
#         print('loss', loss)   #tensor(1.0321

#         print(F.softmax(logits, dim=1))
        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)   # tensor([0, 0, 0, 0, 0], device='cuda:0')
        correct = pred.eq(labels)  # pred랑 labels랑 같으면 True, False  #tensor([False, False, True, False,  True], device='cuda:0')

#         correct.sum() #tensor(2, device='cuda:0')
#         correct.sum().item()  #2   
        total_correct += correct.sum().item()  #2
        total_len += len(labels)   #len(labels) <<< #5

        total_loss += loss.item()  #1.0321286916732788
        
        loss.backward()        
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()        
#         model.zero_grad()
    
        if itr % p_itr == 0:
            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format\
                  (epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))
            total_loss = 0
            total_len = 0
            total_correct = 0
        itr+=1
        
        model_epoch = 'epoch_{}.pt'.format(epoch+1)
#         print(model_epoch)
        final_save_dir = os.path.join(model_save_dir, model_epoch)
#         print(final_save_dir)
#         os.mkdir(final_save_dir)
        
        torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
        }, final_save_dir)
        
    print("=> {}/{} saving checkpoint".format(epoch+1, epochs))

        


# logits tensor([[ 0.3057,  0.1581, -0.0858],
#         [ 0.3702,  0.1425, -0.0284],
#         [ 0.3616,  0.2723,  0.0265],
#         [ 0.2145,  0.1623, -0.0140],
#         [ 0.3651,  0.1810, -0.0185]], device='cuda:0', grad_fn=<AddmmBackward>)

# loss tensor(1.0321, device='cuda:0', grad_fn=<NllLossBackward>)

# # print(F.softmax(logits, dim=1))
# tensor([[0.3939, 0.3398, 0.2663],
#         [0.4052, 0.3227, 0.2720],
#         [0.3803, 0.3478, 0.2720],
#         [0.3643, 0.3458, 0.2899],
#         [0.3979, 0.3310, 0.2711]], device='cuda:0', grad_fn=<SoftmaxBackward>)

# pred tensor([0, 0, 0, 0, 0], device='cuda:0')

# correct tensor([False, False,  True, False,  True], device='cuda:0')




def test():
    model.eval()

    total_loss = 0
    total_len = 0
    total_correct = 0

    for text, label in valid_loader:
#         print("Running Validation...")

        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]
        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]
        
        sample = torch.tensor(padded_list)
        sample, label = sample.to(device), label.to(device)
        labels = label.clone().detach()
        outputs = model(sample, labels=labels)
        _, logits = outputs

        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)
        correct = pred.eq(labels)
        total_correct += correct.sum().item()
        total_len += len(labels)

    print('Test accuracy: ', total_correct / total_len)




#main
start_epoch=0
for epoch in range(start_epoch, epochs):
    train(epoch)
#     save_checkpoint(model_save_dir, {
#         'epoch': epoch,
#         'model': model,
#         'state_dict': model.state_dict(),
#         'optimizer': optimizer.state_dict(),
#     })
    test()

train_date = datetime.today().strftime("%Y-%m-%d %H:%M:%S")
print(train_date)

now = time.gmtime(time.time() - start_time)
print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))

print("Training complete!")
