from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"


import pickle
# path="/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/generated_file/swda_dailydialog_texts_augment.bin"
path='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/generated_file/swda_dailydialog_texts_augment.bin'
with open(path, 'rb') as f:
    reader5=pickle.load(f)
print('reader5', len(reader5), reader5[:3]) #('reader5',  117552,

# path = '/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/generated_file/swda_dailydialog_acts_augment.bin'
path='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/generated_file/swda_dailydialog_acts_augment.bin'
with open(path, 'rb') as f:
    reader6 = pickle.load(f)
print('reader6', len(reader6))  #
# reader6[:10]  #['2', '0', '0', '2', '0', '2', '0', '0', '1', '0']



# 특수기호 제거하기
data=[]
for sent in reader5:
    sent=sent.replace("?", "").replace("!", ""). replace(")", "").replace("(", "").replace("...", "").replace("–", "")
    data.append(sent)
print('기호제거', len(data), data[:2])
# reader5 117552 ['부엌에서 냄새가 난다.', '부엌에서 냄새가 난다.[ACT]2[SEP]나는 쓰레기를 버릴 것이다.', '부엌에서 냄새가 난다.[ACT]2[SEP]나는 쓰레기를 버릴 것이다.[ACT]0[SEP]그래']
# reader6 117552
# 기호제거 117552 ['부엌에서 냄새가 난다.', '부엌에서 냄새가 난다.[ACT]2[SEP]나는 쓰레기를 버릴 것이다.']


# 라벨->넘파이로
import numpy as np
acts = np.array(reader6, dtype=np.float64)
labeling_ix = acts
print('라벨', len(labeling_ix), labeling_ix)




#512이하로 자르기
from tqdm import tqdm
input_sentences = []

for text in tqdm(data):
    text=text.split("[ACT]")
#     print(text)

    while len(str(text)) >490:
        del text[0]
    text="[ACT]".join(text)
    input_sentences.append(text)

print("input_sentences", len(input_sentences), input_sentences[:5])

for row in input_sentences:
    if len(row) > 512:
        print(row)
        
        


import tensorflow as tf
import torch

from transformers import BertTokenizer    # 토크나이저
from transformers import BertForSequenceClassification, AdamW, BertConfig
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
import random
import time
import datetime


import torch
import os

os.environ["CUDA_VISIBLE_DEVICES"] = "0" # Uses GPU 0.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
print('Available device : ', torch.cuda.device_count())
print('Current cuda device :', torch.cuda.current_device())
print(torch.cuda.get_device_name(device))

# BERT input 512이하로 안잘라도됨 -> stride=256, return_overflowing_tokens=True
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', stride=256, return_overflowing_tokens=True, do_lower_case=False)
num_label=3
model = BertForSequenceClassification.from_pretrained("bert-base-multilingual-cased", num_labels=num_label)

# 스페셜 토큰 ACT, TOP 추가
special_tokens_dict = {"additional_special_tokens": [ "[ACT]", "[TOP]"] }
num_added_special_tokens = tokenizer.add_special_tokens(special_tokens_dict)
print('sepecial_tokens', num_added_special_tokens)

# new토큰 추가
new_tokens =["글쎄", "어딨", "어땠", "뭘", "좀", "눈썹", "고맙", "깜빡", "알아챘", "어둡", "칵테일", "둬", "흠", "알잖", "어쨌", "샴푸", "부엌", "긁힌", "펍", "말했", "말썽",
"봅", "비쌉니까", "귤", "짭짤", "덥", "안팎", "뚜껑", "여깄", "여쭤", "눕혀", "볶음", "새우", "아닙", "샵", "젓가락", "숟가락", "압력", "솥", "생선", "찜", "말했",
"배고팠", "땀", "조깅", "짚었", "잤", "여깄", "돼지", "고기", "팁", "예약", "해놨", "바쁠", "깡충", "깡통", "냅킨", "봅", "찐빵", "뵙", "맙소사", "즙", "헷갈",
"뒹굴", "가엾", "찻집", "으깬", "쿵푸", "얌전", "푹", "뽑", "베깅", "핏", "놀랐", "꽉", "춥다", "꽂", "이퀄라이제이션", "놔", "켤레", "벙어리", "샴페인", "쪄", "가오슝",
"씻", "덫", "바빴", "뻣뻣", "찍어", "개", "똥", "엎", "뺄", "볶", "찐", "웍" "놔", "멈췄", "닳", "푹", "찻", "댁", "쪘", "있", "쬐", "잖아"]

num_added_new_tokens  = tokenizer.add_tokens(new_tokens)
print('num_added_new_tokens', num_added_new_tokens)#기쁘다 씻겨   감귤 꽂아 있잖아 쬐었 찻잔 찻주전자



#처음 vocab_size
print('추가 전', model.get_input_embeddings())  #Embedding(119547, 768, padding_idx=0) #tokenizer.vocab_size 

total_added_tokens = num_added_special_tokens + num_added_new_tokens
print('총 추가된 토큰 수', total_added_tokens)

model.resize_token_embeddings(tokenizer.vocab_size + total_added_tokens)
print('토큰 추가 후', model.get_input_embeddings())  #토큰 추가 후 Embedding(119646, 768)


tokenizer.special_tokens_map



# 데이터를 훈련검증셋과 테스트셋으로 분리
from sklearn.model_selection import train_test_split

trainValid_inputs, test_inputs, trainValid_labels, test_labels = train_test_split(input_sentences, #
                                                                                   labeling_ix,
                                                                                    random_state = 2018,
                                                                                    test_size = 0.0085)

print('훈련검증셋', '테스트셋', len(trainValid_inputs), len(trainValid_labels), len(test_inputs), len(test_labels)) #116552 116552 1000 1000




# 훈련검증셋 인풋, 라벨 파일로 저장하기
import pickle
# trainValidInputs_path = "/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/ACT_token/generated_file/speechact_trainValidInputs.bin"
trainValidInputs_path ='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/ACT_token/generated_file/speechact_trainValidInputs.bin'
# with open(trainValidInputs_path, 'wb') as f:
#     pickle.dump(trainValid_inputs, f)
with open(trainValidInputs_path, 'rb') as f:
    trainValidInputs_reader = pickle.load(f)
print('trainValidInputs_reader', len(trainValidInputs_reader), trainValidInputs_reader[:1]) #16552 ['오늘은 무엇을 도와줄까[ACT]1[SEP]세탁기가 고장 났다.[ACT]0[SEP]뭐

# trainValidLabels_path = "/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/ACT_token/generated_file/speechact_trainValidLabels.bin"
trainValidLabels_path ='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/ACT_token/generated_file/speechact_trainValidLabels.bin'
# with open(trainValidLabels_path, 'wb') as f:
#     pickle.dump(trainValid_labels, f)
with open(trainValidLabels_path, 'rb') as f:
    trainValidLabels_reader = pickle.load(f)
print('trainValidLabels_reader', len(trainValidLabels_reader), trainValidLabels_reader[:1])#116552 [1.]

# 테스트 인풋, 라벨 파일로 저장하기
# testInputs_path = "/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/ACT_token/generated_file/speechact_testInputs.bin"
testInputs_path ='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/ACT_token/generated_file/speechact_testInputs.bin'
# with open(testInputs_path, 'wb') as f:
#     pickle.dump(test_inputs, f)
with open(testInputs_path, 'rb') as f:
    testInputs_reader = pickle.load(f)
print('testInputs_reader', len(testInputs_reader), testInputs_reader[:1])
# 1000 ['좋은 오후. 무엇을 도와주시겠습니까[ACT]0[SEP]네. 체크인하고 싶습니다만.[AC

# testLabels_path = "/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/ACT_token/generated_file/speechact_testLabels.bin"
testLabels_path='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/ACT_token/generated_file/speechact_testLabels.bin'
# with open(testLabels_path, 'wb') as f:
#     pickle.dump(test_labels, f)
with open(testLabels_path, 'rb') as f:
    testLabels_reader = pickle.load(f)
print('testLabels_reader', len(testLabels_reader), testLabels_reader[:1])  # 1000 [2.]




# 2. 다시 훈련검증셋을 훈련셋과 검증셋으로 분리
from sklearn.model_selection import train_test_split #93241:23311

train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(trainValid_inputs, #
                                                                                   trainValid_labels, #labeling_ix,
                                                                                    random_state = 2018,
                                                                                    test_size = 0.2)

print('훈련셋', '검증셋', len(train_inputs), len(train_labels), len(validation_inputs), len(validation_labels))
print(train_inputs[:1], validation_inputs[:1], train_labels[:1], validation_labels[:1])

import pickle
# trainInputs_path = "/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/ACT_token/generated_file/speechact_trainInputs.bin"
trainInputs_path ='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/ACT_token/generated_file/speechact_trainInputs.bin'
# with open(trainInputs_path, 'wb') as f:
#     pickle.dump(train_inputs, f)
with open(trainInputs_path, 'rb') as f:
    trainInputs_reader = pickle.load(f)
print('trainInputs_reader', len(trainInputs_reader), trainInputs_reader[:1]) #93241 ['안나, 그 옷 너한테 정말 잘 어울려 그 색깔은 너의 바지와 완벽하

# trainLabels_path = "/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/ACT_token/generated_file/speechact_trainLabels.bin"
trainLabels_path='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/ACT_token/generated_file/speechact_trainLabels.bin'
# with open(trainLabels_path, 'wb') as f:
#     pickle.dump(train_labels, f)
with open(trainLabels_path, 'rb') as f:
    trainLabels_reader = pickle.load(f)
print('trainLabels_reader', len(trainLabels_reader), trainLabels_reader[:1]) #93241 [0.]

#Valid 인풋, 라벨 파일로 저장하기
# validationInputs_path = "/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/ACT_token/generated_file/speechact_validInputs.bin"
validationInputs_path='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/ACT_token/generated_file/speechact_validInputs.bin'
# with open(validationInputs_path, 'wb') as f:
#     pickle.dump(validation_inputs, f)
with open(validationInputs_path, 'rb') as f:
    validationInputs_reader = pickle.load(f)
print('validationInputs_reader', len(validationInputs_reader), validationInputs_reader[:1])
# 23311 ['0[SEP]대략 여든 살 정도 되셨을 거예요.[ACT]0[SEP]근데 정말 좋아 보이셨어요.[ACT]0[SEP]꼿꼿하시고 키도 크셨

# validationLabels_path = "/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/ACT_token/generated_file/speechact_validLabels.bin"
validationLabels_path ='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/ACT_token/generated_file/speechact_validLabels.bin'
# with open(validationLabels_path, 'wb') as f:
#     pickle.dump(validation_labels, f)
with open(validationLabels_path, 'rb') as f:
    validationLabels_reader = pickle.load(f)
print('validationLabels_reader', len(validationLabels_reader), validationLabels_reader[:1])  #  23311 [0.]



# train문장들 입력준비
# #train_inputs
train_sentences = ["[CLS] " + str(sentence) + " [SEP]" for sentence in train_inputs] #sentences
print('train_sentences', train_sentences[:2]) #['[CLS] 안나, 그 옷 너한테 정말 잘 어울려 그 색깔은 너의 바지와 완벽하게 어울린다.[ACT]0[SEP]고

# BERT의 토크나이저로 문장을 토큰으로 분리
# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', stride=256, return_overflowing_tokens=True, do_lower_case=False)
train_tokenizedTexts = [tokenizer.tokenize(sent) for sent in train_sentences]
print ('train_tokenizedTexts', train_tokenizedTexts[:2])
# train_tokenizedTexts[:100]  #[['[CLS]', '안', '##나', ',', '그', '옷', '너', '##한', '##테', '정',

# train 토큰을 숫자로 변환
train_inputIds = [tokenizer.convert_tokens_to_ids(x) for x in train_tokenizedTexts]
print('train_inputIds', train_inputIds[0]) #train_inputIds [[   101   9580 118762  10892   9294 119137  10622   9087  1

# 입력 토큰의 최대 시퀀스 길이
#max_input = 512
max_input = max(len(sen) for sen in train_inputIds)
print(max_input)

# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움
train_inputIds = pad_sequences(train_inputIds, maxlen = max_input, dtype="long", truncating="post", padding="post")
print('train_inputIds', train_inputIds[:1])


# train어텐션 마스크 초기화
train_attentionMasks = []
# 어텐션 마스크를 패딩이면-> 0, 패딩아니면 -> 1로 설정
for seq in train_inputIds: #input_ids
    seq_mask = [float(i>0) for i in seq]
    train_attentionMasks.append(seq_mask)

# float 실수로 반환, 0이면 ->0, 아니면 ->1
# print(train_attentionMasks[0])
print('train_attentionMasks', len(train_attentionMasks), train_attentionMasks[0])  #train_attentionMasks 116552 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1






# validation_inputs
valid_sentences = ["[CLS] " + str(sentence) + " [SEP]" for sentence in validation_inputs] #sentences
valid_sentences[:2] #['[CLS] 0[SEP]대략 여든 살 정도 되셨을 거예요.[ACT]0[SEP]근데 정말 좋아 보이셨어요.[ACT]0[SEP]꼿

# BERT의 토크나이저로 문장을 토큰으로 분리
# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', stride=256, return_overflowing_tokens=True, do_lower_case=False)
valid_tokenizedTexts = [tokenizer.tokenize(sent) for sent in valid_sentences]
print (valid_tokenizedTexts[:2])  #[['[CLS]', '0', '[SEP]', '대', '##략', '여',

# validation 토큰을 숫자로 변환
valid_inputIds = [tokenizer.convert_tokens_to_ids(x) for x in valid_tokenizedTexts]
print(valid_inputIds[0])

# 입력 토큰의 최대 시퀀스 길이
max_input = max(len(sen) for sen in valid_inputIds) # max_input = 512

# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움
valid_inputIds = pad_sequences(valid_inputIds, maxlen = max_input, dtype="long", truncating="post", padding="post")
'valid_inputIds', valid_inputIds[:1]


# valid 어텐션 마스크 초기화
valid_attentionMasks = []
# 어텐션 마스크를 패딩이면-> 0, 패딩아니면 -> 1로 설정
for seq in valid_inputIds: #input_ids
    seq_mask = [float(i>0) for i in seq]
    valid_attentionMasks.append(seq_mask)

# float 실수로 반환, 0이면 ->0, 아니면 ->1
# print(valid_attentionMasks[0])
print('valid_attentionMasks', len(valid_attentionMasks))  #valid_attentionMasks 23311




# 데이터를 파이토치의 텐서로 변환
import torch

train_inputs = torch.tensor(train_inputIds)
train_labels = torch.tensor(train_labels)
# train_labels = torch.tensor(train_labels, dtype=torch.long)
train_masks = torch.tensor(train_attentionMasks)

validation_inputs = torch.tensor(valid_inputIds)
validation_labels = torch.tensor(validation_labels)
validation_masks = torch.tensor(valid_attentionMasks)

print(train_inputs.size(), train_masks.size(), train_labels.size())
print(validation_inputs.size(), validation_masks.size(), validation_labels.size())
# print(test_inputs.size(), test_masks.size(), test_labels.size())
# 타입안맞으면 사이즈 조회안됨. dtype확인

train_inputs=train_inputs[:10]
train_labels=train_labels[:10]
train_masks=train_masks[:10]

# validation_inputs=validation_inputs[:1]
# validation_labels =validation_labels[:1]
# validation_masks=validation_masks[:1]


from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# 저자가 특정task에 파인튜닝시, 추천하는 배치사이즈는 16 or 32,
batch_size = 32

# 데이터셋을 DataLoader로 만들기 (파이토치의 DataLoader로 입력, 라벨, 마스크를 묶어 데이터 설정)
train_data = TensorDataset(train_inputs, train_labels, train_masks)

# 랜덤하게 배치 선택
train_sampler = RandomSampler(train_data)

# 배치사이즈로 학습
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

# train_data
# train_sampler
# train_dataloader
# len(train_dataloader) #5 (9개 데이터에서 배치사이즈2로 5번)


validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)


from transformers import BertForSequenceClassification, AdamW, BertConfig
model.to(device)




from tqdm import tqdm, trange

epochs = 7
total_steps = len(train_dataloader) * epochs
optimizer = AdamW(model.parameters(),
                 lr = 2e-5,
                 eps = 1e-8)


from transformers import get_linear_schedule_with_warmup
# 학습률을 조금씩 감소시키는 스케줄러 생성
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)


# training 학습

# 정확도 계산 함수 # Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):

    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)



# 시간 표시 함수
def format_time(elapsed):
    # 반올림
    elapsed_rounded = int(round((elapsed)))
    # hh:mm:ss으로 형태 변경
    return str(datetime.timedelta(seconds=elapsed_rounded))


import random

# 재현을 위해 랜덤시드 고정
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)


"===================================================== 저장위치 확인 ============================================================================================="
# 그래디언트 초기화
model.zero_grad()
# model_save_dir ="/home/yeji/standard/model/BERT_multilingual/speechact_classification/dataset/3. swda_dailydialog_3acts/ACT_token/_saved_model/20210622"
model_save_dir='/home/yeji/standard/model/speechact_classificaiton/speechact_classification/bert_multilingual/dataset/swda_dailydialog_3acts/ACT_token/saved_model/20210813'





# training_results = []
# f = open("./training_results.txt", 'w')
training_results = []   

date_string = datetime.datetime.now().strftime("%Y-%m-%d-%H:%M")
print('training_data: ', date_string)
file_name= "training_" + date_string

with open(file_name+".txt", 'w') as f:
    # 에폭만큼 반복
    for epoch_i in range(0, epochs):

        # ========================================
        #               Training
        # ========================================
        print("")
        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
        print('Training...')
        f.write("[Epoch {:}/{:}]\n".format(epoch_i + 1, epochs))


        # 시작 시간 설정
        t0 = time.time()

        # 로스 초기화
        total_loss = 0

        # 훈련모드로 변경
        model.train()

        # 데이터로더에서 배치만큼 반복하여 가져옴
        for step, batch in enumerate(train_dataloader):

            # 경과 정보 표시
            if step % 1000 == 0 and not step == 0:
                # if step % 10 == 0 and not step == 0:
                elapsed = format_time(time.time() - t0)
                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

            # 배치를 GPU에 넣음
            batch = tuple(t.to(device) for t in batch)

            # 배치에서 데이터 추출
            #         b_input_ids, b_input_mask, b_labels = batch
            b_input_ids, b_labels, b_input_mask = batch
            #         print('b_labels', b_labels)
            #         print('b_input_ids', b_input_ids)

            # Forward 수행
            outputs = model(b_input_ids,
                            token_type_ids=None,
                            attention_mask=b_input_mask,
                            labels=b_labels.long())

            # 로스 구함
            #         loss = outputs
            loss = outputs[0]
            #         print('loss_'+ str(step), loss) # step4 tensor(1.3582,

            # 총 로스 계산
            total_loss += loss.item()
            #         print('total_loss_' + str(step), total_loss)

            # Backward 수행으로 그래디언트 계산
            loss.backward()

            # 그래디언트 클리핑
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # 그래디언트를 통해 가중치 파라미터 업데이트
            optimizer.step()

            # 스케줄러로 학습률 감소   # Update the learning rate.
            scheduler.step()

            # 그래디언트 초기화
            model.zero_grad()

            # logits = outputs
            logits = outputs[0]

            # CPU로 데이터 이동
            logits = logits.detach().cpu().numpy()
            #         print('트레인라짓', logits)
            label_ids = b_labels.to('cpu').numpy()
        #         print('label_ids', label_ids)


            elem_A = {}
            elem_A['epoch'] = epoch_i + 1
            elem_A['mode'] = "train"
            elem_A['train_loss'] = loss.tolist()
            #             elem_A['eval_acc'] = 0.56

            training_results.append(elem_A)

            write_elem_A = str(elem_A) + '\n'
            f.write(write_elem_A)
    #         print(elem_A)



        # 한 에폭당 평균 로스 계산 (배치당)
        avg_train_loss = total_loss / len(train_dataloader)

        print("")
        print("  Average training loss: {0:.4f}".format(avg_train_loss))
        print("  Training epoch took: {:}".format(format_time(time.time() - t0)))


        model_epoch = 'epoch={}.pt'.format(epoch_i +1)
        final_save_dir = os.path.join(model_save_dir, model_epoch)
        # print('epoch {}, final_save_dir is {}'.format(epoch_i, final_save_dir))
        torch.save({
            'epoch': epoch_i + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict()
        }, final_save_dir)


        # ========================================
        #               Validation
        # ========================================
        print("")
        print("Running Validation...")

        # 시작 시간 설정
        t0 = time.time()

        # 평가모드로 변경
        model.eval()

        # 변수 초기화
        eval_loss, eval_accuracy = 0, 0
        nb_eval_steps, nb_eval_examples = 0, 0

        # 데이터로더에서 배치만큼 반복하여 가져옴
        for batch in validation_dataloader:
            # 배치를 GPU에 넣음
            batch = tuple(t.to(device) for t in batch)

            # 배치에서 데이터 추출
            b_input_ids, b_input_mask, b_labels = batch

            # 그래디언트 계산 안함
            with torch.no_grad():
                # Forward 수행
                outputs = model(b_input_ids,
                                token_type_ids=None,
                                attention_mask=b_input_mask)

                # Get the "logits" output by the model. The "logits" are the output
            # values prior to applying an activation function like the softmax.
            # 로스 구함
            #         logits = outputs
            logits = outputs[0]

            # CPU로 데이터 이동
            logits = logits.detach().cpu().numpy()
            label_ids = b_labels.to('cpu').numpy()

            # 출력 로짓과 라벨을 비교하여 정확도 계산  # Accumulate the total accuracy.
            tmp_eval_accuracy = flat_accuracy(logits, label_ids)
            eval_accuracy += tmp_eval_accuracy
            # Track the number of batches
            nb_eval_steps += 1

            elem_A = {}
            elem_A['epoch'] = epoch_i + 1
            elem_A['mode'] = "eval"
            #             elem_A['train_loss'] = loss
    #         elem_A['eval_acc'] = eval_accuracy / nb_eval_steps
            elem_A['eval_acc'] = "{0:.4f}".format(eval_accuracy / nb_eval_steps)
            training_results.append(elem_A)

            write_elem_A = str(elem_A) + "\n"
            f.write(write_elem_A)
    #         print(write_elem_A)



        print("  Accuracy: {0:.4f}".format(eval_accuracy / nb_eval_steps))
        print("  Validation took: {:}".format(format_time(time.time() - t0)))

    print("")
    print("Training complete!")
    
    
    
    



